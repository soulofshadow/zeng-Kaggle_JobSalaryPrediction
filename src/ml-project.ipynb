{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load & Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T22:59:57.166315Z",
     "iopub.status.busy": "2024-02-18T22:59:57.165564Z",
     "iopub.status.idle": "2024-02-18T22:59:57.171313Z",
     "shell.execute_reply": "2024-02-18T22:59:57.170470Z",
     "shell.execute_reply.started": "2024-02-18T22:59:57.166277Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import gc\n",
    "\n",
    "from functools import reduce\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LocationRaw</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractType</th>\n",
       "      <th>ContractTime</th>\n",
       "      <th>Company</th>\n",
       "      <th>Category</th>\n",
       "      <th>SalaryRaw</th>\n",
       "      <th>SalaryNormalized</th>\n",
       "      <th>SourceName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12612628</td>\n",
       "      <td>Engineering Systems Analyst</td>\n",
       "      <td>Engineering Systems Analyst Dorking Surrey Sal...</td>\n",
       "      <td>Dorking, Surrey, Surrey</td>\n",
       "      <td>Dorking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 30000/annum 20-30K</td>\n",
       "      <td>25000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12612830</td>\n",
       "      <td>Stress Engineer Glasgow</td>\n",
       "      <td>Stress Engineer Glasgow Salary **** to **** We...</td>\n",
       "      <td>Glasgow, Scotland, Scotland</td>\n",
       "      <td>Glasgow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>25000 - 35000/annum 25-35K</td>\n",
       "      <td>30000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12612844</td>\n",
       "      <td>Modelling and simulation analyst</td>\n",
       "      <td>Mathematical Modeller / Simulation Analyst / O...</td>\n",
       "      <td>Hampshire, South East, South East</td>\n",
       "      <td>Hampshire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 40000/annum 20-40K</td>\n",
       "      <td>30000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12613049</td>\n",
       "      <td>Engineering Systems Analyst / Mathematical Mod...</td>\n",
       "      <td>Engineering Systems Analyst / Mathematical Mod...</td>\n",
       "      <td>Surrey, South East, South East</td>\n",
       "      <td>Surrey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>25000 - 30000/annum 25K-30K negotiable</td>\n",
       "      <td>27500</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12613647</td>\n",
       "      <td>Pioneer, Miser Engineering Systems Analyst</td>\n",
       "      <td>Pioneer, Miser  Engineering Systems Analyst Do...</td>\n",
       "      <td>Surrey, South East, South East</td>\n",
       "      <td>Surrey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 30000/annum 20-30K</td>\n",
       "      <td>25000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                                              Title  \\\n",
       "0  12612628                        Engineering Systems Analyst   \n",
       "1  12612830                            Stress Engineer Glasgow   \n",
       "2  12612844                   Modelling and simulation analyst   \n",
       "3  12613049  Engineering Systems Analyst / Mathematical Mod...   \n",
       "4  12613647         Pioneer, Miser Engineering Systems Analyst   \n",
       "\n",
       "                                     FullDescription  \\\n",
       "0  Engineering Systems Analyst Dorking Surrey Sal...   \n",
       "1  Stress Engineer Glasgow Salary **** to **** We...   \n",
       "2  Mathematical Modeller / Simulation Analyst / O...   \n",
       "3  Engineering Systems Analyst / Mathematical Mod...   \n",
       "4  Pioneer, Miser  Engineering Systems Analyst Do...   \n",
       "\n",
       "                         LocationRaw LocationNormalized ContractType  \\\n",
       "0            Dorking, Surrey, Surrey            Dorking          NaN   \n",
       "1        Glasgow, Scotland, Scotland            Glasgow          NaN   \n",
       "2  Hampshire, South East, South East          Hampshire          NaN   \n",
       "3     Surrey, South East, South East             Surrey          NaN   \n",
       "4     Surrey, South East, South East             Surrey          NaN   \n",
       "\n",
       "  ContractTime                       Company          Category  \\\n",
       "0    permanent  Gregory Martin International  Engineering Jobs   \n",
       "1    permanent  Gregory Martin International  Engineering Jobs   \n",
       "2    permanent  Gregory Martin International  Engineering Jobs   \n",
       "3    permanent  Gregory Martin International  Engineering Jobs   \n",
       "4    permanent  Gregory Martin International  Engineering Jobs   \n",
       "\n",
       "                                SalaryRaw  SalaryNormalized        SourceName  \n",
       "0              20000 - 30000/annum 20-30K             25000  cv-library.co.uk  \n",
       "1              25000 - 35000/annum 25-35K             30000  cv-library.co.uk  \n",
       "2              20000 - 40000/annum 20-40K             30000  cv-library.co.uk  \n",
       "3  25000 - 30000/annum 25K-30K negotiable             27500  cv-library.co.uk  \n",
       "4              20000 - 30000/annum 20-30K             25000  cv-library.co.uk  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('./data/Train_rev1.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      "RangeIndex: 244768 entries, 0 to 244767\n",
      "\n",
      "Data columns (total 12 columns):\n",
      "\n",
      " #   Column              Non-Null Count   Dtype \n",
      "\n",
      "---  ------              --------------   ----- \n",
      "\n",
      " 0   Id                  244768 non-null  int64 \n",
      "\n",
      " 1   Title               244767 non-null  object\n",
      "\n",
      " 2   FullDescription     244768 non-null  object\n",
      "\n",
      " 3   LocationRaw         244768 non-null  object\n",
      "\n",
      " 4   LocationNormalized  244768 non-null  object\n",
      "\n",
      " 5   ContractType        65442 non-null   object\n",
      "\n",
      " 6   ContractTime        180863 non-null  object\n",
      "\n",
      " 7   Company             212338 non-null  object\n",
      "\n",
      " 8   Category            244768 non-null  object\n",
      "\n",
      " 9   SalaryRaw           244768 non-null  object\n",
      "\n",
      " 10  SalaryNormalized    244768 non-null  int64 \n",
      "\n",
      " 11  SourceName          244767 non-null  object\n",
      "\n",
      "dtypes: int64(2), object(10)\n",
      "\n",
      "memory usage: 22.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Most of the data type are not numeric which requires us for further processing:**\n",
    "  - These \"objects\" represent texts (information) about the job advertisement.\n",
    "  - for columns like **'Category', 'ContractType', 'ContractTime'** we can use simple encoder like one-hot encoder.\n",
    "  - for columns like **'Title', 'FullDescription'** we might need some NLP techniques' help for better embedding them and get better representing of these feature.\n",
    "- **Our target:**\n",
    "  - **SalaryNormalized:** is a numerical feature which will remain the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MissingNum</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Title</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FullDescription</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LocationRaw</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LocationNormalized</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ContractType</th>\n",
       "      <td>179326</td>\n",
       "      <td>73.263662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ContractTime</th>\n",
       "      <td>63905</td>\n",
       "      <td>26.108397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company</th>\n",
       "      <td>32430</td>\n",
       "      <td>13.249281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SalaryRaw</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SalaryNormalized</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SourceName</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    MissingNum  Percentage\n",
       "Id                           0    0.000000\n",
       "Title                        1    0.000409\n",
       "FullDescription              0    0.000000\n",
       "LocationRaw                  0    0.000000\n",
       "LocationNormalized           0    0.000000\n",
       "ContractType            179326   73.263662\n",
       "ContractTime             63905   26.108397\n",
       "Company                  32430   13.249281\n",
       "Category                     0    0.000000\n",
       "SalaryRaw                    0    0.000000\n",
       "SalaryNormalized             0    0.000000\n",
       "SourceName                   1    0.000409"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check missing data \n",
    "missing_data = df_train.isnull().sum()\n",
    "missing_data = pd.DataFrame(missing_data, columns=['MissingNum'])\n",
    "missing_data['Percentage'] = missing_data/len(df_train)*100\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First issue pointed out is the **'Huge' missing of 'ContractType'** which we might consider drop this feature.\n",
    "- While for the features **ContractTime**, and **Company** we might consider adding a third category as 'unknown' to impute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Count</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UK</td>\n",
       "      <td>41093</td>\n",
       "      <td>16.788551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>London</td>\n",
       "      <td>30522</td>\n",
       "      <td>12.469767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>South East London</td>\n",
       "      <td>11713</td>\n",
       "      <td>4.785348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The City</td>\n",
       "      <td>6678</td>\n",
       "      <td>2.728298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Manchester</td>\n",
       "      <td>3516</td>\n",
       "      <td>1.436462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Leeds</td>\n",
       "      <td>3401</td>\n",
       "      <td>1.389479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Birmingham</td>\n",
       "      <td>3061</td>\n",
       "      <td>1.250572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Central London</td>\n",
       "      <td>2607</td>\n",
       "      <td>1.065090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>West Midlands</td>\n",
       "      <td>2540</td>\n",
       "      <td>1.037717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Surrey</td>\n",
       "      <td>2397</td>\n",
       "      <td>0.979295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Location  Count  Percentage\n",
       "0                 UK  41093   16.788551\n",
       "1             London  30522   12.469767\n",
       "2  South East London  11713    4.785348\n",
       "3           The City   6678    2.728298\n",
       "4         Manchester   3516    1.436462\n",
       "5              Leeds   3401    1.389479\n",
       "6         Birmingham   3061    1.250572\n",
       "7     Central London   2607    1.065090\n",
       "8      West Midlands   2540    1.037717\n",
       "9             Surrey   2397    0.979295"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check locations\n",
    "location_data = Counter(df_train.LocationNormalized)\n",
    "location_data_top10 = location_data.most_common(10)\n",
    "location_data_top10 = pd.DataFrame(location_data_top10, columns=['Location','Count'])\n",
    "location_data_top10['Percentage'] = location_data_top10['Count']/len(df_train)*100\n",
    "location_data_top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Even though LocationNormalized don't have missing data, **But**,\n",
    "    - What is 'The City'? Why 'UK'(united) compare with 'West Midlands'(area), 'Surrey'(county), 'Leeds'(city).\n",
    "    - For better utilize the location information we might need to pre-process and clean it by ourselves.\n",
    "    - Adzuna's normalised location from their own location tree, and they claimed This normaliser is not perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Contract</th>\n",
       "      <th>Count</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>permanent</td>\n",
       "      <td>151521</td>\n",
       "      <td>61.903925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>63905</td>\n",
       "      <td>26.108397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>contract</td>\n",
       "      <td>29342</td>\n",
       "      <td>11.987678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Contract   Count  Percentage\n",
       "0  permanent  151521   61.903925\n",
       "1        NaN   63905   26.108397\n",
       "2   contract   29342   11.987678"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check contract\n",
    "contract_data = Counter(df_train.ContractTime).most_common()\n",
    "contract_data = pd.DataFrame(contract_data, columns=['Contract','Count'])\n",
    "contract_data['Percentage'] = contract_data['Count']/len(df_train)*100\n",
    "contract_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can simply just sign 'unknown' as a third type to NaN for fixing this missing issue\n",
    "- Even adding 'unknown' there will only have 3 types, which using one-hot to encode is also fine (Sparse matrix concern)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after EDA, we release the memory of the data\n",
    "del missing_data\n",
    "del location_data\n",
    "del contract_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Location Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import urllib\n",
    "import simplejson as json\n",
    "\n",
    "#API keys\n",
    "DOMAIN = 'http://api.geonames.org/'\n",
    "USERNAME = 'soulofshadow' \n",
    "VALID_KWARGS = ('q', 'name', 'name_equals', 'name_startsWith', 'maxRows', 'startRow', \n",
    "                'country', 'countryBias', 'continentCode', \n",
    "                'adminCode1', 'adminCode2', 'adminCode3', 'featureClass', 'featureCode', \n",
    "                'lang', 'type', 'style', 'isNameRequired', 'tag', 'operator', 'charset',)\n",
    "\n",
    "# 'LocationRaw' has a lot of data with only Country or a Direction,\n",
    "# so we need to filter the data to get the most accurate data\n",
    "# here i set the capital if only the country is in the data\n",
    "# and set the first city of that Direction in **ENGLAND** if the data is a direction\n",
    "IGNORE_SIGNS = ['UK', 'GB', 'United Kingdom', 'Great Britain', 'England', 'Scotland', 'Wales', 'Northern Ireland']\n",
    "IGNORE_ENGLAND = ['North England', \n",
    "                   'North West England', 'North East England',\n",
    "                   'South England', \n",
    "                   'South West England', 'South East England', \n",
    "                   'East England', 'East of England',\n",
    "                   'West England', 'West of England',\n",
    "                   'Midlands', \n",
    "                   'East Midlands', 'West Midlands']\n",
    "IGNORE_DIRECTIONS = ['North', 'South', 'East', 'West', \n",
    "                     'North West', 'North East', \n",
    "                     'South West', 'South East']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationRetrieval():\n",
    "    def __init__(self, low_request_dict):\n",
    "        #save a search result to directly use so less api request\n",
    "        self.low_request_dict = low_request_dict\n",
    "\n",
    "    def fetchJson(self, method, params):\n",
    "        uri = DOMAIN + '%s?%s&username=%s' % (method, urllib.parse.urlencode(params), USERNAME)\n",
    "        resource = urllib.request.urlopen(uri).readlines()\n",
    "        js = json.loads(resource[0])\n",
    "        return js\n",
    "\n",
    "    #this is the code for the api request\n",
    "    def search(self, **kwargs):\n",
    "        method = 'searchJSON'\n",
    "        valid_kwargs = VALID_KWARGS\n",
    "        params = {}\n",
    "        custom_params = {'country': 'GB', 'maxRows': 1, 'lang': 'en', 'style': 'FULL', 'featureClass': 'P'}\n",
    "        params.update(custom_params)\n",
    "\n",
    "        for key in kwargs:\n",
    "            if key in valid_kwargs:\n",
    "                params[key] = kwargs[key]\n",
    "            \n",
    "        # fuzzy search mode, include worldwide location\n",
    "        if kwargs['fuzzy']:\n",
    "            del params['maxRows']\n",
    "            del params['country']\n",
    "            del params['featureClass']\n",
    "            params['fuzzy'] = 0.7\n",
    "    \n",
    "        results = self.fetchJson(method, params)\n",
    "        if('geonames' in results):\n",
    "            return results['geonames']\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    #this is the main function to get the result of the location\n",
    "    def check_search_and_save(self, q, fuzzy=False):\n",
    "\n",
    "        #check if already searched in past\n",
    "        if q in self.low_request_dict:\n",
    "            return (self.low_request_dict[q]['Country'], self.low_request_dict[q]['County'], self.low_request_dict[q]['City'])\n",
    "        \n",
    "        #search\n",
    "        results = self.search(q=q, fuzzy=fuzzy)\n",
    "        if not results:\n",
    "            return False\n",
    "        else:\n",
    "            result = results[0]\n",
    "            if len(results) > 1:\n",
    "                #get the info of UK's\n",
    "                for r in results:\n",
    "                    if r['adminName1'] and r['countryCode'] == 'GB':\n",
    "                        result = r\n",
    "                        break\n",
    "            \n",
    "            #fclName means the type of location\n",
    "            if result['fclName'] == 'parks,area, ...':\n",
    "                if result['fcodeName'] == 'continent':\n",
    "                    Country = County = City = -1\n",
    "                else:\n",
    "                    Country = result['adminName1']\n",
    "                    County = City = result['name']\n",
    "            elif result['fclName'] == 'country, state, region,...':\n",
    "                Country = County = City = -1\n",
    "            elif result['fclName'] == 'mountain,hill,rock,... ':\n",
    "                Country = result['adminName1']\n",
    "                County = City = result['name']\n",
    "            else:\n",
    "                #another country\n",
    "                if result['adminName1'] and result['countryCode'] != 'GB':\n",
    "                    Country = result['countryName']\n",
    "                    County = result['adminName1']\n",
    "                    City = result['name']\n",
    "                #GB situation (best situation)\n",
    "                else:\n",
    "                    Country = result['adminName1']\n",
    "                    County = result['adminName2']\n",
    "                    City = result['name']\n",
    "        \n",
    "        if Country and County and City:\n",
    "            #save\n",
    "            self.low_request_dict[q] = {'Country': Country, 'County': County, 'City': City}\n",
    "            return (Country, County, City)\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "def is_sign_to_ignore(q, ignores):\n",
    "    for ignore in ignores:\n",
    "        if q.lower() == ignore.lower():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def contains_london(q):\n",
    "    pattern = re.compile(r'london', re.IGNORECASE)\n",
    "    return bool(pattern.search(q))\n",
    "\n",
    "def contains_keys(q, keys: dict):\n",
    "    keys = list(keys.keys())\n",
    "    for key in keys:\n",
    "        if key.lower() in q.lower():\n",
    "            return key\n",
    "    return None\n",
    "\n",
    "# check if 'LocationRaw' is valid for search\n",
    "def has_english_letters(input_string):\n",
    "    return bool(re.search(r'[a-zA-Z]', input_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_location_raw = pd.DataFrame({'LocationRaw': df_train['LocationRaw']})\n",
    "low_request_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save for debug\n",
    "debug_missing = []\n",
    "\n",
    "retriver = LocationRetrieval(low_request_dict)\n",
    "\n",
    "tqdm.pandas()\n",
    "for index, row in tqdm(df_location_raw.iterrows(), total=len(df_location_raw)):\n",
    "\n",
    "    #define flags\n",
    "    flag = False #find correct result\n",
    "    Missing = False #Original data is incorrect or not in the database\n",
    "    if not has_english_letters(row['LocationRaw']):\n",
    "        continue\n",
    "\n",
    "    elements = row['LocationRaw'].lower()\n",
    "    elements = re.split(',|/|_|&', elements)\n",
    "    #1. find from the splited list of str (ignore countries and directions situation)\n",
    "    if not flag and len(elements) > 1:\n",
    "        for element in elements:\n",
    "            q = element.strip()\n",
    "            if len(q) == 1 or is_sign_to_ignore(q, IGNORE_SIGNS) or is_sign_to_ignore(q, IGNORE_ENGLAND) or is_sign_to_ignore(q, IGNORE_DIRECTIONS):\n",
    "                continue\n",
    "            if contains_london(q):\n",
    "                q = 'london'\n",
    "            #first check\n",
    "            flag = retriver.check_search_and_save(q)\n",
    "            if flag: \n",
    "                break\n",
    "            #second check with fuzzy\n",
    "            flag = retriver.check_search_and_save(q, fuzzy=True)\n",
    "            if flag: \n",
    "                break\n",
    "\n",
    "    #2. we include the countries and directions situation to search again\n",
    "    if not flag:\n",
    "        for element in elements:\n",
    "            q = element.strip()\n",
    "            if contains_keys(q, low_request_dict):\n",
    "                q = contains_keys(q, low_request_dict)\n",
    "                flag = retriver.check_search_and_save(q)\n",
    "            elif is_sign_to_ignore(q, IGNORE_SIGNS):\n",
    "                flag = retriver.check_search_and_save(q)\n",
    "            elif is_sign_to_ignore(q, IGNORE_ENGLAND):\n",
    "                flag = retriver.check_search_and_save(q, fuzzy=True)\n",
    "            elif is_sign_to_ignore(q, IGNORE_DIRECTIONS):\n",
    "                flag = retriver.check_search_and_save(q)\n",
    "                \n",
    "    #3. we open fuzzy search which will search not only UK but worldwide\n",
    "    if not flag:\n",
    "        element = elements[0]\n",
    "        element = element.strip()\n",
    "        #fuzzy separate search\n",
    "        elements = re.split(' ', element)\n",
    "        #check\n",
    "        for element in elements:\n",
    "            q = element.strip()\n",
    "            if contains_keys(q, low_request_dict):\n",
    "                q = contains_keys(q, low_request_dict)\n",
    "                flag = retriver.check_search_and_save(q)\n",
    "                break\n",
    "        #search\n",
    "        if not flag:\n",
    "            for element in elements:\n",
    "                q = element.strip()\n",
    "                flag = retriver.check_search_and_save(q)\n",
    "                if flag: \n",
    "                    break\n",
    "                \n",
    "    if flag:\n",
    "        df_location_raw.loc[index, 'Country'] = flag[0]\n",
    "        df_location_raw.loc[index, 'County'] = flag[1]\n",
    "        df_location_raw.loc[index, 'City'] = flag[2]\n",
    "\n",
    "    if not flag:\n",
    "        print(index)\n",
    "        debug_missing.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save for next use\n",
    "with open('./data/Location_output.json', 'w') as json_file:\n",
    "    json.dump(low_request_dict, json_file, indent=2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_location_raw.replace(-1, np.nan, inplace=True)\n",
    "df_location_raw = df_location_raw.drop('LocationRaw', axis=1)\n",
    "df_location_raw.to_csv('Train_location.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with our calibrated location information\n",
    "df_train_with_location = pd.concat([df_train, df_location_raw], axis=1)\n",
    "\n",
    "#reshape columns order\n",
    "columns_index = ['Title', 'FullDescription', 'Company', 'Country', 'County', 'City', 'ContractTime', 'Category', 'SalaryNormalized']\n",
    "df_train_ordered = df_train_with_location[columns_index]\n",
    "\n",
    "#fill missing data with 'unknown' for 'ContractTime'\n",
    "df_train_ordered.loc[:,'ContractTime'].replace(np.nan, \"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ordered.to_csv('Train_Missing_Fixed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text combine and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_fields(row):\n",
    "    return (f\"{row['Title']}\\n\"\n",
    "            f\"Location: {row['Country']}, {row['County']}, {row['City']}\\n\"\n",
    "            f\"Contract Time: {row['ContractTime']}\\n\"\n",
    "            f\"Company: {row['Company']}\\n\"\n",
    "            f\"Category: {row['Category']}\\n\"\n",
    "            f\"Description:\\n{row['FullDescription']}\\n\")\n",
    "    \n",
    "def remove_end(txt):\n",
    "    txt = txt[:txt.rfind('—')]\n",
    "    return re.sub('—.+?, [0-9]+','',txt)\n",
    "\n",
    "def remove_urls(txt):\n",
    "    txt = re.sub(r'html\\S+','',txt)\n",
    "    txt = re.sub(r'http\\S+','',txt)\n",
    "    txt = re.sub(r'pic.\\S+','',txt)\n",
    "    txt = re.sub(r'www.\\S+','',txt)\n",
    "    return txt\n",
    "\n",
    "def remove_escaped(txt):\n",
    "    return re.sub(r'&\\S+','',txt)\n",
    "\n",
    "def text_preprocess(txt):\n",
    "    text_process_list = [remove_escaped, remove_urls, remove_end]\n",
    "    txt = reduce(lambda txt,func: func(txt),text_process_list,txt)\n",
    "    return txt\n",
    "\n",
    "def get_processed_df(df):\n",
    "    for i, row in df.iterrows():\n",
    "        full_text = concatenate_fields(row)\n",
    "        full_text = text_preprocess(full_text)\n",
    "        df.loc[i, 'FullText'] = full_text\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_processed = get_processed_df(df_train_ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_processed.to_csv('Train_Text_Processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T23:03:33.692578Z",
     "iopub.status.busy": "2024-02-18T23:03:33.692204Z",
     "iopub.status.idle": "2024-02-18T23:03:33.697551Z",
     "shell.execute_reply": "2024-02-18T23:03:33.696535Z",
     "shell.execute_reply.started": "2024-02-18T23:03:33.692544Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_device_and_set_seed(seed):\n",
    "    \"\"\" Set all seeds to make results reproducible \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    np.random.seed(seed)\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    return device\n",
    "\n",
    "SEED = 123\n",
    "device = get_device_and_set_seed(SEED)\n",
    "\n",
    "sentence_model_name = 'MohammedDhiyaEddine/job-skill-sentence-transformer-tsdae'\n",
    "sentence_model = SentenceTransformer(sentence_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get text embedding from pretrained model\n",
    "def get_sentence_embedding(model, sentences):\n",
    "    batch_size = 128\n",
    "    for batch in tqdm(range(0, len(sentences), batch_size)):\n",
    "        if batch + batch_size < len(sentences):\n",
    "            X_text_batch = sentences[batch:batch+batch_size]\n",
    "        else:\n",
    "            X_text_batch = sentences[batch:]\n",
    "\n",
    "        if batch == 0:\n",
    "            X_text_emb = model.encode(X_text_batch, convert_to_tensor=True)\n",
    "        else:\n",
    "            X_text_emb = torch.cat((X_text_emb, model.encode(X_text_batch, convert_to_tensor=True)), 0)\n",
    "\n",
    "    return X_text_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244768/244768 [30:06<00:00, 135.50it/s] \n"
     ]
    }
   ],
   "source": [
    "text_embeddings = get_sentence_embedding(sentence_model, df_train_processed['FullText'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T23:02:18.281602Z",
     "iopub.status.busy": "2024-02-18T23:02:18.281244Z",
     "iopub.status.idle": "2024-02-18T23:02:48.598563Z",
     "shell.execute_reply": "2024-02-18T23:02:48.597665Z",
     "shell.execute_reply.started": "2024-02-18T23:02:18.281573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We gonna have 391 classes for salary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34/4193637675.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  targets.loc[:, 'mapped_salary'] = targets['SalaryNormalized'].apply(map_to_nearest_range)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Design:\n",
    "1. simple NN from text embeddings to salary\n",
    "2. split salary to classes and use a classification model with cross entropy loss\n",
    "3. \n",
    "'''\n",
    "text_embeddings = torch.load('/kaggle/input/ml-project-dataset/text_embed.pt')\n",
    "df = pd.read_csv('/kaggle/input/job-salary-predict/Train_Text_Processed.csv')\n",
    "targets = df[['SalaryNormalized']]\n",
    "\n",
    "#Divide the original salary into k intervals\n",
    "salary_range = list(range(5000, 200001, 500))\n",
    "print(f'We gonna have {len(salary_range)} classes for salary.')\n",
    "\n",
    "#assign the closest interval to each sample\n",
    "salary_range_dict = {value: index for index, value in enumerate(salary_range)}\n",
    "salary_range_dict_reverse = {index: value for index, value in enumerate(salary_range)}\n",
    "def map_to_nearest_range(salary):\n",
    "    return salary_range_dict[min(salary_range, key=lambda x: abs(x - salary))]\n",
    "\n",
    "targets.loc[:, 'mapped_salary'] = targets['SalaryNormalized'].apply(map_to_nearest_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T23:02:58.036907Z",
     "iopub.status.busy": "2024-02-18T23:02:58.036537Z",
     "iopub.status.idle": "2024-02-18T23:02:58.113169Z",
     "shell.execute_reply": "2024-02-18T23:02:58.112166Z",
     "shell.execute_reply.started": "2024-02-18T23:02:58.036878Z"
    }
   },
   "outputs": [],
   "source": [
    "#Split \n",
    "X_train, X_test, df_y_train, df_y_test = train_test_split(text_embeddings, targets, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T23:05:59.502220Z",
     "iopub.status.busy": "2024-02-18T23:05:59.501527Z",
     "iopub.status.idle": "2024-02-18T23:05:59.507622Z",
     "shell.execute_reply": "2024-02-18T23:05:59.506687Z",
     "shell.execute_reply.started": "2024-02-18T23:05:59.502186Z"
    }
   },
   "outputs": [],
   "source": [
    "#put it into tensor \n",
    "y_train = torch.from_numpy(df_y_train['SalaryNormalized'].values).long().to(device)\n",
    "y_test = torch.from_numpy(df_y_test['SalaryNormalized'].values).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T23:06:14.184750Z",
     "iopub.status.busy": "2024-02-18T23:06:14.184421Z",
     "iopub.status.idle": "2024-02-18T23:06:14.229986Z",
     "shell.execute_reply": "2024-02-18T23:06:14.229147Z",
     "shell.execute_reply.started": "2024-02-18T23:06:14.184725Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.regression = nn.Sequential(\n",
    "            nn.Linear(768, 1536),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1536, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(768, 384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(384, 384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(384, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.regression(x)\n",
    "    \n",
    "model = Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T23:07:17.267702Z",
     "iopub.status.busy": "2024-02-18T23:07:17.266862Z",
     "iopub.status.idle": "2024-02-18T23:07:17.273079Z",
     "shell.execute_reply": "2024-02-18T23:07:17.272173Z",
     "shell.execute_reply.started": "2024-02-18T23:07:17.267667Z"
    }
   },
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()\n",
    "mae = nn.L1Loss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "epochs = 300\n",
    "batch_size = 128\n",
    "\n",
    "lowest_mae = 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T23:07:23.699382Z",
     "iopub.status.busy": "2024-02-18T23:07:23.698414Z",
     "iopub.status.idle": "2024-02-18T23:22:11.006783Z",
     "shell.execute_reply": "2024-02-18T23:22:11.005652Z",
     "shell.execute_reply.started": "2024-02-18T23:07:23.699345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "MAE: 9981.1943359375, Epoch: 0\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 9410.0927734375, Epoch: 1\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 8991.494140625, Epoch: 2\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 8811.890625, Epoch: 3\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 8627.8994140625, Epoch: 4\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 8490.45703125, Epoch: 5\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 8384.01953125, Epoch: 6\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 8304.2470703125, Epoch: 7\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 8232.9091796875, Epoch: 8\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 8143.30322265625, Epoch: 9\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 8064.83544921875, Epoch: 10\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7982.14453125, Epoch: 11\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7920.2060546875, Epoch: 12\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7868.17041015625, Epoch: 13\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7807.25830078125, Epoch: 14\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7765.4052734375, Epoch: 15\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7715.1787109375, Epoch: 16\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7697.927734375, Epoch: 17\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7648.845703125, Epoch: 18\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7576.84619140625, Epoch: 19\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7560.11083984375, Epoch: 21\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7543.3271484375, Epoch: 22\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7519.14697265625, Epoch: 23\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7506.18994140625, Epoch: 24\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7427.63330078125, Epoch: 27\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7400.7548828125, Epoch: 28\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7383.16650390625, Epoch: 30\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7362.66455078125, Epoch: 31\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7325.7763671875, Epoch: 32\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7254.67138671875, Epoch: 37\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7130.1103515625, Epoch: 38\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7127.86328125, Epoch: 41\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7124.1435546875, Epoch: 42\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7120.6025390625, Epoch: 43\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7100.05908203125, Epoch: 44\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7065.4365234375, Epoch: 45\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7007.76806640625, Epoch: 46\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6984.1748046875, Epoch: 48\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6979.875, Epoch: 50\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6910.0546875, Epoch: 52\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6865.75634765625, Epoch: 54\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6837.8583984375, Epoch: 58\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6789.4482421875, Epoch: 59\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6771.361328125, Epoch: 62\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6755.80712890625, Epoch: 63\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6741.25244140625, Epoch: 66\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6732.2119140625, Epoch: 67\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6702.90771484375, Epoch: 73\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6650.3681640625, Epoch: 77\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6641.76513671875, Epoch: 83\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6605.75927734375, Epoch: 84\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6605.5400390625, Epoch: 86\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6563.5810546875, Epoch: 87\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6534.994140625, Epoch: 89\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6517.822265625, Epoch: 96\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6510.083984375, Epoch: 103\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6476.5205078125, Epoch: 115\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6455.5439453125, Epoch: 119\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6417.05224609375, Epoch: 121\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6400.92041015625, Epoch: 151\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6344.78955078125, Epoch: 156\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6318.81103515625, Epoch: 162\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6276.9169921875, Epoch: 187\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6247.46484375, Epoch: 189\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6223.40576171875, Epoch: 202\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6209.93505859375, Epoch: 207\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6168.009765625, Epoch: 213\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6154.5322265625, Epoch: 274\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6152.86572265625, Epoch: 275\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6151.18896484375, Epoch: 276\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6113.9638671875, Epoch: 278\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6090.01318359375, Epoch: 282\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6050.2080078125, Epoch: 284\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6033.84130859375, Epoch: 286\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6011.24658203125, Epoch: 287\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 5998.39794921875, Epoch: 298\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in range(0, len(X_train), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = X_train[batch:batch+batch_size]\n",
    "        labels = y_train[batch:batch+batch_size]\n",
    "\n",
    "        output = model(inputs)\n",
    "        loss = mse(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(X_test)\n",
    "        loss = mae(output.squeeze(), y_test.float())\n",
    "        if loss < lowest_mae:\n",
    "            lowest_mae = loss\n",
    "            print('--------------------------------------------------')\n",
    "            print(f'MAE: {loss}, Epoch: {epoch}')\n",
    "            print('--------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T23:25:26.337815Z",
     "iopub.status.busy": "2024-02-18T23:25:26.337055Z",
     "iopub.status.idle": "2024-02-18T23:25:26.360382Z",
     "shell.execute_reply": "2024-02-18T23:25:26.359508Z",
     "shell.execute_reply.started": "2024-02-18T23:25:26.337765Z"
    }
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "\n",
    "torch.save(model.state_dict(), 'model_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T23:26:03.130724Z",
     "iopub.status.busy": "2024-02-18T23:26:03.130342Z",
     "iopub.status.idle": "2024-02-18T23:26:03.136846Z",
     "shell.execute_reply": "2024-02-18T23:26:03.135839Z",
     "shell.execute_reply.started": "2024-02-18T23:26:03.130675Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = torch.from_numpy(df_y_train['mapped_salary'].values).long().to(device)\n",
    "y_test = torch.from_numpy(df_y_test['SalaryNormalized'].values).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T23:44:47.953680Z",
     "iopub.status.busy": "2024-02-18T23:44:47.953014Z",
     "iopub.status.idle": "2024-02-18T23:44:48.074887Z",
     "shell.execute_reply": "2024-02-18T23:44:48.074153Z",
     "shell.execute_reply.started": "2024-02-18T23:44:47.953647Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model_Mapped(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.classification = nn.Sequential(\n",
    "            nn.Linear(768, 3072),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(3072, 1536),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1536, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(768, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(768, 391)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classification(x)\n",
    "    \n",
    "model_mapped = Model_Mapped().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T23:45:03.236589Z",
     "iopub.status.busy": "2024-02-18T23:45:03.235886Z",
     "iopub.status.idle": "2024-02-18T23:45:03.241901Z",
     "shell.execute_reply": "2024-02-18T23:45:03.240768Z",
     "shell.execute_reply.started": "2024-02-18T23:45:03.236549Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model_mapped.parameters(), lr=5e-5)\n",
    "\n",
    "epochs = 300\n",
    "batch_size = 128\n",
    "\n",
    "lowest_mae = 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T23:46:04.726964Z",
     "iopub.status.busy": "2024-02-18T23:46:04.726621Z",
     "iopub.status.idle": "2024-02-19T00:12:23.357254Z",
     "shell.execute_reply": "2024-02-19T00:12:23.356447Z",
     "shell.execute_reply.started": "2024-02-18T23:46:04.726938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "MAE: 7875.09130859375, Epoch: 0\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7715.4267578125, Epoch: 1\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7493.73681640625, Epoch: 2\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7482.83203125, Epoch: 3\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7383.83447265625, Epoch: 4\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7246.47509765625, Epoch: 5\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7169.50048828125, Epoch: 6\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 7017.37890625, Epoch: 7\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6854.94580078125, Epoch: 10\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6829.29443359375, Epoch: 12\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6761.75634765625, Epoch: 14\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6684.90673828125, Epoch: 15\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6629.537109375, Epoch: 18\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6595.4638671875, Epoch: 22\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6590.76611328125, Epoch: 31\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6394.06201171875, Epoch: 33\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6310.15234375, Epoch: 34\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6290.08935546875, Epoch: 44\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6201.66650390625, Epoch: 45\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6192.4384765625, Epoch: 46\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6110.10107421875, Epoch: 47\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6073.44775390625, Epoch: 50\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6033.30419921875, Epoch: 53\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 6030.5966796875, Epoch: 171\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 5992.916015625, Epoch: 187\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 5974.71484375, Epoch: 257\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "MAE: 5971.31982421875, Epoch: 276\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model_mapped.train()\n",
    "    for batch in range(0, len(X_train), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = X_train[batch:batch+batch_size]\n",
    "        labels = y_train[batch:batch+batch_size]\n",
    "\n",
    "        output = model_mapped(inputs)\n",
    "        loss = cross_entropy(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model_mapped.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model_mapped(X_test)\n",
    "        preds = output.argmax(dim=1)\n",
    "        preds_value = torch.tensor([salary_range_dict_reverse[key.item()] for key in preds]).to(device)\n",
    "        loss = mae(preds_value, y_test.float())\n",
    "        if loss < lowest_mae:\n",
    "            lowest_mae = loss\n",
    "            print('--------------------------------------------------')\n",
    "            print(f'MAE: {loss}, Epoch: {epoch}')\n",
    "            print('--------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_mapped.state_dict(), 'model_mapped_state_dict.pth')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4458335,
     "sourceId": 7648242,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4460906,
     "sourceId": 7651916,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
